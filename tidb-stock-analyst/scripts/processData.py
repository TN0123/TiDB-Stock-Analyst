from sentence_transformers import SentenceTransformer
import os
from tidb_vector.integrations import TiDBVectorClient
from dotenv import load_dotenv
import sys
import json


# downloads model for text embeddings
embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
embed_model_dims = embed_model.get_sentence_embedding_dimension()

# Generates embeddings for the headlines when scraping
def text_to_embedding(text):
    embedding = embed_model.encode(text)
    return embedding.tolist()

# Loads the connection string from the .env file
load_dotenv()

vector_store = TiDBVectorClient(
   # The table which will store the vector data.
   table_name='embedded_documents',
   # The connection string to the TiDB cluster.
   connection_string=os.environ.get('TIDB_DATABASE_URL'),
   # The dimension of the vector generated by the embedding model.
   vector_dimension=embed_model_dims,
)

# Shows the result of querying the top headlines closest to the prompt
def print_result(query, result):
   print(f"Search result (\"{query}\"):")
   for r in result:
      print(f"- text: \"{r.document}\", distance: {r.distance}")

def processResult(result):
    for r in result:
        print(r.document + "," + r.metadata)

# Behavior when this script is called
if len(sys.argv) == 3: # Puts data into DB
    documents = json.loads(sys.argv[2])
    vector_store.insert(
        texts=[doc["title"] for doc in documents],
        embeddings=[text_to_embedding(doc["title"]) for doc in documents],
        metadatas=[doc["link"] for doc in documents],
    )
elif len(sys.argv) == 2:   # Fetches data from DB
    query = sys.argv[1]
    query_embedding = text_to_embedding(query)
    search_result = vector_store.query(query_embedding, k=3)
    # print(search_result)
    # print_result(query, search_result)
    (processResult(search_result))
    